{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfS0tObYvVJ2",
        "outputId": "061407b8-a9a2-4bbc-c72d-e9dd473f2bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- Experience_Years: string (nullable = true)\n",
            " |-- Age: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Salary: string (nullable = true)\n",
            "\n",
            "+---+----------------+---+------+------+\n",
            "| ID|Experience_Years|Age|Gender|Salary|\n",
            "+---+----------------+---+------+------+\n",
            "|  1|               5| 28|Female|250000|\n",
            "|  2|               1| 21|  Male| 50000|\n",
            "|  3|               3| 23|Female|170000|\n",
            "|  4|               2| 22|  Male| 25000|\n",
            "|  5|               1| 17|  Male| 10000|\n",
            "+---+----------------+---+------+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"EmployeeSalarySQLApp\").getOrCreate()\n",
        "df = spark.read.option(\"header\", True).csv(\"/Employee_Salary_Dataset.csv\")\n",
        "\n",
        "# result = spark.sql(\"SELECT * FROM employee_salary\")\n",
        "df.printSchema()\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "val df = spark.read\n",
        "  .option(\"header\", true)\n",
        "  .csv(\"/Employee_Salary_Dataset.csv\")\n",
        "\n",
        "df.printSchema()\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "vyq9U9df8_bl",
        "outputId": "17a9cd0c-a40c-4928-b3bb-030e103af73e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1756431673.py, line 4)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1756431673.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    val df = spark.read\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),(\"Michael\",\"Rose\",\"USA\",\"NY\"), \\\n",
        "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),(\"Maria\",\"Jones\",\"USA\",\"FL\") \\\n",
        "  ]\n",
        "columns=[\"firstname\",\"lastname\",\"country\",\"state\"]\n",
        "df=spark.createDataFrame(data=data,schema=columns)\n",
        "df.show()\n",
        "print(df.collect())\n",
        "\n",
        "states1=df.rdd.map(lambda x: x[3]).collect()\n",
        "print(states1)\n",
        "#['CA', 'NY', 'CA', 'FL']\n",
        "from collections import OrderedDict\n",
        "res = list(OrderedDict.fromkeys(states1))\n",
        "print(res)\n",
        "#['CA', 'NY', 'FL']\n",
        "\n",
        "\n",
        "#Example 2\n",
        "states2=df.rdd.map(lambda x: x.state).collect()\n",
        "print(states2)\n",
        "#['CA', 'NY', 'CA', 'FL']\n",
        "\n",
        "states3=df.select(df.state).collect()\n",
        "print(states3)\n",
        "#[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n",
        "\n",
        "states4=df.select(df.state).rdd.flatMap(lambda x: x).collect()\n",
        "print(states4)\n",
        "#['CA', 'NY', 'CA', 'FL']\n",
        "\n",
        "states5=df.select(df.state).toPandas()['state']\n",
        "states6=list(states5)\n",
        "print(states6)\n",
        "#['CA', 'NY', 'CA', 'FL']\n",
        "\n",
        "pandDF=df.select(df.state,df.firstname).toPandas()\n",
        "print(list(pandDF['state']))\n",
        "print(list(pandDF['firstname']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TVQi7XqHNF4",
        "outputId": "294608a4-3a14-4537-d51c-7d75d2ed47e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+-------+-----+\n",
            "|firstname|lastname|country|state|\n",
            "+---------+--------+-------+-----+\n",
            "|    James|   Smith|    USA|   CA|\n",
            "|  Michael|    Rose|    USA|   NY|\n",
            "|   Robert|Williams|    USA|   CA|\n",
            "|    Maria|   Jones|    USA|   FL|\n",
            "+---------+--------+-------+-----+\n",
            "\n",
            "[Row(firstname='James', lastname='Smith', country='USA', state='CA'), Row(firstname='Michael', lastname='Rose', country='USA', state='NY'), Row(firstname='Robert', lastname='Williams', country='USA', state='CA'), Row(firstname='Maria', lastname='Jones', country='USA', state='FL')]\n",
            "['CA', 'NY', 'CA', 'FL']\n",
            "['CA', 'NY', 'FL']\n",
            "['CA', 'NY', 'CA', 'FL']\n",
            "[Row(state='CA'), Row(state='NY'), Row(state='CA'), Row(state='FL')]\n",
            "['CA', 'NY', 'CA', 'FL']\n",
            "['CA', 'NY', 'CA', 'FL']\n",
            "['CA', 'NY', 'CA', 'FL']\n",
            "['James', 'Michael', 'Robert', 'Maria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import to_timestamp, current_timestamp\n",
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"seq\", StringType(), True)\n",
        "])\n",
        "\n",
        "dates = [(\"1\",)]\n",
        "\n",
        "df = spark.createDataFrame(dates, schema=schema)\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEUHZmd8HU7_",
        "outputId": "4e2b1ca3-1c75-43f4-c6bd-1c7e0a8ebc66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "|seq|\n",
            "+---+\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]]\n",
        "\n",
        "# Create the pandas DataFrame\n",
        "pandasDF = pd.DataFrame(data, columns = ['Name', 'Age'])\n",
        "\n",
        "# print dataframe.\n",
        "print(pandasDF)\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[1]\") \\\n",
        "    .appName(\"SparkByExamples.com\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sparkDF=spark.createDataFrame(pandasDF)\n",
        "sparkDF.printSchema()\n",
        "sparkDF.show()\n",
        "\n",
        "#sparkDF=spark.createDataFrame(pandasDF.astype(str))\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
        "mySchema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
        "                       ,StructField(\"Age\", IntegerType(), True)])\n",
        "\n",
        "sparkDF2 = spark.createDataFrame(pandasDF,schema=mySchema)\n",
        "sparkDF2.printSchema()\n",
        "sparkDF2.show()\n",
        "\n",
        "\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.fallback.enabled\",\"true\")\n",
        "\n",
        "pandasDF2=sparkDF2.select(\"*\").toPandas\n",
        "print(pandasDF2)\n",
        "\n",
        "\n",
        "test=spark.conf.get(\"spark.sql.execution.arrow.enabled\")\n",
        "print(test)\n",
        "\n",
        "test123=spark.conf.get(\"spark.sql.execution.arrow.pyspark.fallback.enabled\")\n",
        "print(test123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuIR86hGIDq8",
        "outputId": "e6cce308-7d66-493b-d6b3-b9b745ee6c26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Name  Age\n",
            "0   Scott   50\n",
            "1    Jeff   45\n",
            "2  Thomas   54\n",
            "3     Ann   34\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            "\n",
            "+------+---+\n",
            "|  Name|Age|\n",
            "+------+---+\n",
            "| Scott| 50|\n",
            "|  Jeff| 45|\n",
            "|Thomas| 54|\n",
            "|   Ann| 34|\n",
            "+------+---+\n",
            "\n",
            "root\n",
            " |-- First Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            "\n",
            "+----------+---+\n",
            "|First Name|Age|\n",
            "+----------+---+\n",
            "|     Scott| 50|\n",
            "|      Jeff| 45|\n",
            "|    Thomas| 54|\n",
            "|       Ann| 34|\n",
            "+----------+---+\n",
            "\n",
            "<bound method PandasConversionMixin.toPandas of DataFrame[First Name: string, Age: int]>\n",
            "true\n",
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "from pyspark.sql.functions import col,expr\n",
        "data=[(\"2019-01-23\",1),(\"2019-06-24\",2),(\"2019-09-20\",3)]\n",
        "spark.createDataFrame(data).toDF(\"date\",\"increment\") \\\n",
        "    .select(col(\"date\"),col(\"increment\"), \\\n",
        "      expr(\"add_months(to_date(date,'yyyy-MM-dd'),cast(increment as int))\").alias(\"inc_date\")) \\\n",
        "    .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0LDQw3OIKAO",
        "outputId": "9cbf3908-c3bc-4d13-9095-1797017f8ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+----------+\n",
            "|      date|increment|  inc_date|\n",
            "+----------+---------+----------+\n",
            "|2019-01-23|        1|2019-02-23|\n",
            "|2019-06-24|        2|2019-08-24|\n",
            "|2019-09-20|        3|2019-12-20|\n",
            "+----------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "data = [('James','Smith','M',3000),\n",
        "  ('Anna','Rose','F',4100),\n",
        "  ('Robert','Williams','M',6200),\n",
        "]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
        "df = spark.createDataFrame(data=data, schema = columns)\n",
        "df.show()\n",
        "\n",
        "\n",
        "if 'salary1' not in df.columns:\n",
        "    print(\"aa\")\n",
        "\n",
        "# Add new constanct column\n",
        "from pyspark.sql.functions import lit\n",
        "df.withColumn(\"bonus_percent\", lit(0.3)) \\\n",
        "  .show()\n",
        "\n",
        "#Add column from existing column\n",
        "df.withColumn(\"bonus_amount\", df.salary*0.3) \\\n",
        "  .show()\n",
        "\n",
        "#Add column by concatinating existing columns\n",
        "from pyspark.sql.functions import concat_ws\n",
        "df.withColumn(\"name\", concat_ws(\" \",\"firstname\",'lastname')) \\\n",
        "  .show()\n",
        "\n",
        "#Add current date\n",
        "from pyspark.sql.functions import current_date\n",
        "df.withColumn(\"current_date\", current_date()) \\\n",
        "  .show()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "df.withColumn(\"grade\", \\\n",
        "   when((df.salary < 4000), lit(\"A\")) \\\n",
        "     .when((df.salary >= 4000) & (df.salary <= 5000), lit(\"B\")) \\\n",
        "     .otherwise(lit(\"C\")) \\\n",
        "  ).show()\n",
        "\n",
        "# Add column using select\n",
        "df.select(\"firstname\",\"salary\", lit(0.3).alias(\"bonus\")).show()\n",
        "df.select(\"firstname\",\"salary\", lit(df.salary * 0.3).alias(\"bonus_amount\")).show()\n",
        "df.select(\"firstname\",\"salary\", current_date().alias(\"today_date\")).show()\n",
        "\n",
        "#Add columns using SQL\n",
        "df.createOrReplaceTempView(\"PER\")\n",
        "spark.sql(\"select firstname,salary, '0.3' as bonus from PER\").show()\n",
        "spark.sql(\"select firstname,salary, salary * 0.3 as bonus_amount from PER\").show()\n",
        "spark.sql(\"select firstname,salary, current_date() as today_date from PER\").show()\n",
        "spark.sql(\"select firstname,salary, \" +\n",
        "          \"case salary when salary < 4000 then 'A' \"+\n",
        "          \"else 'B' END as grade from PER\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXk_A_5EIWDZ",
        "outputId": "b0832427-0b7a-475a-c21b-ded8f5dfecad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+------+------+\n",
            "|firstname|lastname|gender|salary|\n",
            "+---------+--------+------+------+\n",
            "|    James|   Smith|     M|  3000|\n",
            "|     Anna|    Rose|     F|  4100|\n",
            "|   Robert|Williams|     M|  6200|\n",
            "+---------+--------+------+------+\n",
            "\n",
            "aa\n",
            "+---------+--------+------+------+-------------+\n",
            "|firstname|lastname|gender|salary|bonus_percent|\n",
            "+---------+--------+------+------+-------------+\n",
            "|    James|   Smith|     M|  3000|          0.3|\n",
            "|     Anna|    Rose|     F|  4100|          0.3|\n",
            "|   Robert|Williams|     M|  6200|          0.3|\n",
            "+---------+--------+------+------+-------------+\n",
            "\n",
            "+---------+--------+------+------+------------+\n",
            "|firstname|lastname|gender|salary|bonus_amount|\n",
            "+---------+--------+------+------+------------+\n",
            "|    James|   Smith|     M|  3000|       900.0|\n",
            "|     Anna|    Rose|     F|  4100|      1230.0|\n",
            "|   Robert|Williams|     M|  6200|      1860.0|\n",
            "+---------+--------+------+------+------------+\n",
            "\n",
            "+---------+--------+------+------+---------------+\n",
            "|firstname|lastname|gender|salary|           name|\n",
            "+---------+--------+------+------+---------------+\n",
            "|    James|   Smith|     M|  3000|    James Smith|\n",
            "|     Anna|    Rose|     F|  4100|      Anna Rose|\n",
            "|   Robert|Williams|     M|  6200|Robert Williams|\n",
            "+---------+--------+------+------+---------------+\n",
            "\n",
            "+---------+--------+------+------+------------+\n",
            "|firstname|lastname|gender|salary|current_date|\n",
            "+---------+--------+------+------+------------+\n",
            "|    James|   Smith|     M|  3000|  2025-08-01|\n",
            "|     Anna|    Rose|     F|  4100|  2025-08-01|\n",
            "|   Robert|Williams|     M|  6200|  2025-08-01|\n",
            "+---------+--------+------+------+------------+\n",
            "\n",
            "+---------+--------+------+------+-----+\n",
            "|firstname|lastname|gender|salary|grade|\n",
            "+---------+--------+------+------+-----+\n",
            "|    James|   Smith|     M|  3000|    A|\n",
            "|     Anna|    Rose|     F|  4100|    B|\n",
            "|   Robert|Williams|     M|  6200|    C|\n",
            "+---------+--------+------+------+-----+\n",
            "\n",
            "+---------+------+-----+\n",
            "|firstname|salary|bonus|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|  0.3|\n",
            "|     Anna|  4100|  0.3|\n",
            "|   Robert|  6200|  0.3|\n",
            "+---------+------+-----+\n",
            "\n",
            "+---------+------+------------+\n",
            "|firstname|salary|bonus_amount|\n",
            "+---------+------+------------+\n",
            "|    James|  3000|       900.0|\n",
            "|     Anna|  4100|      1230.0|\n",
            "|   Robert|  6200|      1860.0|\n",
            "+---------+------+------------+\n",
            "\n",
            "+---------+------+----------+\n",
            "|firstname|salary|today_date|\n",
            "+---------+------+----------+\n",
            "|    James|  3000|2025-08-01|\n",
            "|     Anna|  4100|2025-08-01|\n",
            "|   Robert|  6200|2025-08-01|\n",
            "+---------+------+----------+\n",
            "\n",
            "+---------+------+-----+\n",
            "|firstname|salary|bonus|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|  0.3|\n",
            "|     Anna|  4100|  0.3|\n",
            "|   Robert|  6200|  0.3|\n",
            "+---------+------+-----+\n",
            "\n",
            "+---------+------+------------+\n",
            "|firstname|salary|bonus_amount|\n",
            "+---------+------+------------+\n",
            "|    James|  3000|       900.0|\n",
            "|     Anna|  4100|      1230.0|\n",
            "|   Robert|  6200|      1860.0|\n",
            "+---------+------+------------+\n",
            "\n",
            "+---------+------+----------+\n",
            "|firstname|salary|today_date|\n",
            "+---------+------+----------+\n",
            "|    James|  3000|2025-08-01|\n",
            "|     Anna|  4100|2025-08-01|\n",
            "|   Robert|  6200|2025-08-01|\n",
            "+---------+------+----------+\n",
            "\n",
            "+---------+------+-----+\n",
            "|firstname|salary|grade|\n",
            "+---------+------+-----+\n",
            "|    James|  3000|    B|\n",
            "|     Anna|  4100|    B|\n",
            "|   Robert|  6200|    B|\n",
            "+---------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import approx_count_distinct,collect_list\n",
        "from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n",
        "from pyspark.sql.functions import first, last, kurtosis, min, mean, skewness\n",
        "from pyspark.sql.functions import stddev, stddev_samp, stddev_pop, sumDistinct\n",
        "from pyspark.sql.functions import variance,var_samp,  var_pop\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "simpleData = [(\"James\", \"Sales\", 3000),\n",
        "    (\"Michael\", \"Sales\", 4600),\n",
        "    (\"Robert\", \"Sales\", 4100),\n",
        "    (\"Maria\", \"Finance\", 3000),\n",
        "    (\"James\", \"Sales\", 3000),\n",
        "    (\"Scott\", \"Finance\", 3300),\n",
        "    (\"Jen\", \"Finance\", 3900),\n",
        "    (\"Jeff\", \"Marketing\", 3000),\n",
        "    (\"Kumar\", \"Marketing\", 2000),\n",
        "    (\"Saif\", \"Sales\", 4100)\n",
        "  ]\n",
        "schema = [\"employee_name\", \"department\", \"salary\"]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data=simpleData, schema = schema)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "print(\"approx_count_distinct: \" + \\\n",
        "      str(df.select(approx_count_distinct(\"salary\")).collect()[0][0]))\n",
        "\n",
        "print(\"avg: \" + str(df.select(avg(\"salary\")).collect()[0][0]))\n",
        "\n",
        "df.select(collect_list(\"salary\")).show(truncate=False)\n",
        "\n",
        "df.select(collect_set(\"salary\")).show(truncate=False)\n",
        "\n",
        "df2 = df.select(countDistinct(\"department\", \"salary\"))\n",
        "df2.show(truncate=False)\n",
        "print(\"Distinct Count of Department &amp; Salary: \"+str(df2.collect()[0][0]))\n",
        "\n",
        "print(\"count: \"+str(df.select(count(\"salary\")).collect()[0]))\n",
        "df.select(first(\"salary\")).show(truncate=False)\n",
        "df.select(last(\"salary\")).show(truncate=False)\n",
        "df.select(kurtosis(\"salary\")).show(truncate=False)\n",
        "df.select(max(\"salary\")).show(truncate=False)\n",
        "df.select(min(\"salary\")).show(truncate=False)\n",
        "df.select(mean(\"salary\")).show(truncate=False)\n",
        "df.select(skewness(\"salary\")).show(truncate=False)\n",
        "df.select(stddev(\"salary\"), stddev_samp(\"salary\"), \\\n",
        "    stddev_pop(\"salary\")).show(truncate=False)\n",
        "df.select(sum(\"salary\")).show(truncate=False)\n",
        "df.select(sumDistinct(\"salary\")).show(truncate=False)\n",
        "df.select(variance(\"salary\"),var_samp(\"salary\"),var_pop(\"salary\")) \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6P7ZItCIdnw",
        "outputId": "865da994-f917-4e28-c41a-49ecfc456071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n",
            "approx_count_distinct: 6\n",
            "avg: 3400.0\n",
            "+------------------------------------------------------------+\n",
            "|collect_list(salary)                                        |\n",
            "+------------------------------------------------------------+\n",
            "|[3000, 4600, 4100, 3000, 3000, 3300, 3900, 3000, 2000, 4100]|\n",
            "+------------------------------------------------------------+\n",
            "\n",
            "+------------------------------------+\n",
            "|collect_set(salary)                 |\n",
            "+------------------------------------+\n",
            "|[4600, 3000, 3900, 4100, 3300, 2000]|\n",
            "+------------------------------------+\n",
            "\n",
            "+----------------------------------+\n",
            "|count(DISTINCT department, salary)|\n",
            "+----------------------------------+\n",
            "|8                                 |\n",
            "+----------------------------------+\n",
            "\n",
            "Distinct Count of Department &amp; Salary: 8\n",
            "count: Row(count(salary)=10)\n",
            "+-------------+\n",
            "|first(salary)|\n",
            "+-------------+\n",
            "|3000         |\n",
            "+-------------+\n",
            "\n",
            "+------------+\n",
            "|last(salary)|\n",
            "+------------+\n",
            "|4100        |\n",
            "+------------+\n",
            "\n",
            "+-------------------+\n",
            "|kurtosis(salary)   |\n",
            "+-------------------+\n",
            "|-0.6467803030303032|\n",
            "+-------------------+\n",
            "\n",
            "+-----------+\n",
            "|max(salary)|\n",
            "+-----------+\n",
            "|4600       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|min(salary)|\n",
            "+-----------+\n",
            "|2000       |\n",
            "+-----------+\n",
            "\n",
            "+-----------+\n",
            "|avg(salary)|\n",
            "+-----------+\n",
            "|3400.0     |\n",
            "+-----------+\n",
            "\n",
            "+--------------------+\n",
            "|skewness(salary)    |\n",
            "+--------------------+\n",
            "|-0.12041791181069571|\n",
            "+--------------------+\n",
            "\n",
            "+-----------------+-------------------+------------------+\n",
            "|stddev(salary)   |stddev_samp(salary)|stddev_pop(salary)|\n",
            "+-----------------+-------------------+------------------+\n",
            "|765.9416862050705|765.9416862050705  |726.636084983398  |\n",
            "+-----------------+-------------------+------------------+\n",
            "\n",
            "+-----------+\n",
            "|sum(salary)|\n",
            "+-----------+\n",
            "|34000      |\n",
            "+-----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/functions.py:988: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n",
            "  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|sum(DISTINCT salary)|\n",
            "+--------------------+\n",
            "|20900               |\n",
            "+--------------------+\n",
            "\n",
            "+-----------------+-----------------+---------------+\n",
            "|var_samp(salary) |var_samp(salary) |var_pop(salary)|\n",
            "+-----------------+-----------------+---------------+\n",
            "|586666.6666666666|586666.6666666666|528000.0       |\n",
            "+-----------------+-----------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
        "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
        "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
        "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import col, concat_ws\n",
        "df2 = df.withColumn(\"languagesAtSchool\",\n",
        "   concat_ws(\",\",col(\"languagesAtSchool\")))\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False)\n",
        "\n",
        "\n",
        "df.createOrReplaceTempView(\"ARRAY_STRING\")\n",
        "spark.sql(\"select name, concat_ws(',',languagesAtSchool) as languagesAtSchool,currentState from ARRAY_STRING\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emahm4OFItAD",
        "outputId": "7b4b5374-8d32-4a87-cef1-3d485eb8db84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- currentState: string (nullable = true)\n",
            "\n",
            "+----------------+------------------+------------+\n",
            "|name            |languagesAtSchool |currentState|\n",
            "+----------------+------------------+------------+\n",
            "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
            "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
            "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
            "+----------------+------------------+------------+\n",
            "\n",
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: string (nullable = false)\n",
            " |-- currentState: string (nullable = true)\n",
            "\n",
            "+----------------+-----------------+------------+\n",
            "|name            |languagesAtSchool|currentState|\n",
            "+----------------+-----------------+------------+\n",
            "|James,,Smith    |Java,Scala,C++   |CA          |\n",
            "|Michael,Rose,   |Spark,Java,C++   |NJ          |\n",
            "|Robert,,Williams|CSharp,VB        |NV          |\n",
            "+----------------+-----------------+------------+\n",
            "\n",
            "+----------------+-----------------+------------+\n",
            "|name            |languagesAtSchool|currentState|\n",
            "+----------------+-----------------+------------+\n",
            "|James,,Smith    |Java,Scala,C++   |CA          |\n",
            "|Michael,Rose,   |Spark,Java,C++   |NJ          |\n",
            "|Robert,,Williams|CSharp,VB        |NV          |\n",
            "+----------------+-----------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "columns = [\"name\",\"languagesAtSchool\",\"currentState\"]\n",
        "data = [(\"James,,Smith\",[\"Java\",\"Scala\",\"C++\"],\"CA\"), \\\n",
        "    (\"Michael,Rose,\",[\"Spark\",\"Java\",\"C++\"],\"NJ\"), \\\n",
        "    (\"Robert,,Williams\",[\"CSharp\",\"VB\"],\"NV\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "#Flatmap\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yePanYuGaOm3",
        "outputId": "1c093155-3aba-4a45-f223-dab06ca72772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- languagesAtSchool: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- currentState: string (nullable = true)\n",
            "\n",
            "+----------------+------------------+------------+\n",
            "|name            |languagesAtSchool |currentState|\n",
            "+----------------+------------------+------------+\n",
            "|James,,Smith    |[Java, Scala, C++]|CA          |\n",
            "|Michael,Rose,   |[Spark, Java, C++]|NJ          |\n",
            "|Robert,,Williams|[CSharp, VB]      |NV          |\n",
            "+----------------+------------------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "               .appName('SparkByExamples.com') \\\n",
        "               .getOrCreate()\n",
        "data=[[\"1\",\"2020-02-01\"],[\"2\",\"2019-03-01\"],[\"3\",\"2021-03-01\"]]\n",
        "df=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "df.show()\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "#current_date()\n",
        "df.select(current_date().alias(\"current_date\")\n",
        "  ).show(1)\n",
        "\n",
        "#date_format()\n",
        "df.select(col(\"input\"),\n",
        "    date_format(col(\"input\"), \"MM-dd-yyyy\").alias(\"date_format\")\n",
        "  ).show()\n",
        "\n",
        "#to_date()\n",
        "df.select(col(\"input\"),\n",
        "    to_date(col(\"input\"), \"yyy-MM-dd\").alias(\"to_date\")\n",
        "  ).show()\n",
        "\n",
        "#datediff()\n",
        "df.select(col(\"input\"),\n",
        "    datediff(current_date(),col(\"input\")).alias(\"datediff\")\n",
        "  ).show()\n",
        "\n",
        "#months_between()\n",
        "df.select(col(\"input\"),\n",
        "    months_between(current_date(),col(\"input\")).alias(\"months_between\")\n",
        "  ).show()\n",
        "\n",
        "#trunc()\n",
        "df.select(col(\"input\"),\n",
        "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\"),\n",
        "    trunc(col(\"input\"),\"Year\").alias(\"Month_Year\"),\n",
        "    trunc(col(\"input\"),\"Month\").alias(\"Month_Trunc\")\n",
        "   ).show()\n",
        "\n",
        "#add_months() , date_add(), date_sub()\n",
        "\n",
        "df.select(col(\"input\"),\n",
        "    add_months(col(\"input\"),3).alias(\"add_months\"),\n",
        "    add_months(col(\"input\"),-3).alias(\"sub_months\"),\n",
        "    date_add(col(\"input\"),4).alias(\"date_add\"),\n",
        "    date_sub(col(\"input\"),4).alias(\"date_sub\")\n",
        "  ).show()\n",
        "\n",
        "#\n",
        "\n",
        "df.select(col(\"input\"),\n",
        "     year(col(\"input\")).alias(\"year\"),\n",
        "     month(col(\"input\")).alias(\"month\"),\n",
        "     next_day(col(\"input\"),\"Sunday\").alias(\"next_day\"),\n",
        "     weekofyear(col(\"input\")).alias(\"weekofyear\")\n",
        "  ).show()\n",
        "\n",
        "df.select(col(\"input\"),\n",
        "     dayofweek(col(\"input\")).alias(\"dayofweek\"),\n",
        "     dayofmonth(col(\"input\")).alias(\"dayofmonth\"),\n",
        "     dayofyear(col(\"input\")).alias(\"dayofyear\"),\n",
        "  ).show()\n",
        "\n",
        "data=[[\"1\",\"02-01-2020 11 01 19 06\"],[\"2\",\"03-01-2019 12 01 19 406\"],[\"3\",\"03-01-2021 12 01 19 406\"]]\n",
        "df2=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "df2.show(truncate=False)\n",
        "\n",
        "#current_timestamp()\n",
        "df2.select(current_timestamp().alias(\"current_timestamp\")\n",
        "  ).show(1,truncate=False)\n",
        "\n",
        "#to_timestamp()\n",
        "df2.select(col(\"input\"),\n",
        "    to_timestamp(col(\"input\"), \"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")\n",
        "  ).show(truncate=False)\n",
        "\n",
        "\n",
        "#hour, minute,second\n",
        "data=[[\"1\",\"2020-02-01 11:01:19.06\"],[\"2\",\"2019-03-01 12:01:19.406\"],[\"3\",\"2021-03-01 12:01:19.406\"]]\n",
        "df3=spark.createDataFrame(data,[\"id\",\"input\"])\n",
        "\n",
        "df3.select(col(\"input\"),\n",
        "    hour(col(\"input\")).alias(\"hour\"),\n",
        "    minute(col(\"input\")).alias(\"minute\"),\n",
        "    second(col(\"input\")).alias(\"second\")\n",
        "  ).show(truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQhFVS_ka3A1",
        "outputId": "d137fe85-7361-46da-b694-0cbdf3a5ffaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+\n",
            "| id|     input|\n",
            "+---+----------+\n",
            "|  1|2020-02-01|\n",
            "|  2|2019-03-01|\n",
            "|  3|2021-03-01|\n",
            "+---+----------+\n",
            "\n",
            "+------------+\n",
            "|current_date|\n",
            "+------------+\n",
            "|  2025-08-01|\n",
            "+------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+----------+-----------+\n",
            "|     input|date_format|\n",
            "+----------+-----------+\n",
            "|2020-02-01| 02-01-2020|\n",
            "|2019-03-01| 03-01-2019|\n",
            "|2021-03-01| 03-01-2021|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+----------+\n",
            "|     input|   to_date|\n",
            "+----------+----------+\n",
            "|2020-02-01|2020-02-01|\n",
            "|2019-03-01|2019-03-01|\n",
            "|2021-03-01|2021-03-01|\n",
            "+----------+----------+\n",
            "\n",
            "+----------+--------+\n",
            "|     input|datediff|\n",
            "+----------+--------+\n",
            "|2020-02-01|    2008|\n",
            "|2019-03-01|    2345|\n",
            "|2021-03-01|    1614|\n",
            "+----------+--------+\n",
            "\n",
            "+----------+--------------+\n",
            "|     input|months_between|\n",
            "+----------+--------------+\n",
            "|2020-02-01|          66.0|\n",
            "|2019-03-01|          77.0|\n",
            "|2021-03-01|          53.0|\n",
            "+----------+--------------+\n",
            "\n",
            "+----------+-----------+----------+-----------+\n",
            "|     input|Month_Trunc|Month_Year|Month_Trunc|\n",
            "+----------+-----------+----------+-----------+\n",
            "|2020-02-01| 2020-02-01|2020-01-01| 2020-02-01|\n",
            "|2019-03-01| 2019-03-01|2019-01-01| 2019-03-01|\n",
            "|2021-03-01| 2021-03-01|2021-01-01| 2021-03-01|\n",
            "+----------+-----------+----------+-----------+\n",
            "\n",
            "+----------+----------+----------+----------+----------+\n",
            "|     input|add_months|sub_months|  date_add|  date_sub|\n",
            "+----------+----------+----------+----------+----------+\n",
            "|2020-02-01|2020-05-01|2019-11-01|2020-02-05|2020-01-28|\n",
            "|2019-03-01|2019-06-01|2018-12-01|2019-03-05|2019-02-25|\n",
            "|2021-03-01|2021-06-01|2020-12-01|2021-03-05|2021-02-25|\n",
            "+----------+----------+----------+----------+----------+\n",
            "\n",
            "+----------+----+-----+----------+----------+\n",
            "|     input|year|month|  next_day|weekofyear|\n",
            "+----------+----+-----+----------+----------+\n",
            "|2020-02-01|2020|    2|2020-02-02|         5|\n",
            "|2019-03-01|2019|    3|2019-03-03|         9|\n",
            "|2021-03-01|2021|    3|2021-03-07|         9|\n",
            "+----------+----+-----+----------+----------+\n",
            "\n",
            "+----------+---------+----------+---------+\n",
            "|     input|dayofweek|dayofmonth|dayofyear|\n",
            "+----------+---------+----------+---------+\n",
            "|2020-02-01|        7|         1|       32|\n",
            "|2019-03-01|        6|         1|       60|\n",
            "|2021-03-01|        2|         1|       60|\n",
            "+----------+---------+----------+---------+\n",
            "\n",
            "+---+-----------------------+\n",
            "|id |input                  |\n",
            "+---+-----------------------+\n",
            "|1  |02-01-2020 11 01 19 06 |\n",
            "|2  |03-01-2019 12 01 19 406|\n",
            "|3  |03-01-2021 12 01 19 406|\n",
            "+---+-----------------------+\n",
            "\n",
            "+--------------------------+\n",
            "|current_timestamp         |\n",
            "+--------------------------+\n",
            "|2025-08-01 06:57:39.550657|\n",
            "+--------------------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+-----------------------+-----------------------+\n",
            "|input                  |to_timestamp           |\n",
            "+-----------------------+-----------------------+\n",
            "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
            "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
            "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
            "+-----------------------+-----------------------+\n",
            "\n",
            "+-----------------------+----+------+------+\n",
            "|input                  |hour|minute|second|\n",
            "+-----------------------+----+------+------+\n",
            "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
            "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
            "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
            "+-----------------------+----+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import org.apache.spark.sql.SparkSession,\n",
        "val spark = SparkSession.builder().master(\"local[1]\").appName(\"SparkByExamples.com\").getOrCreate()\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "pd.read_csv(io.StringIO('''\n",
        "\n",
        "# File location and type\n",
        "file_location = \"/content/sample_data/mnist_train_small.csv\"\n",
        "file_type = \"csv\"\n",
        "\n",
        "# CSV options\n",
        "infer_schema = \"false\"\n",
        "first_row_is_header = \"\"false\"\n",
        "delimiter = \" \"\n",
        "\n",
        "\"# The applied options are for CSV files. For other file types, these will be ignored.\"\n",
        "df = spark.read.format(file_type) \\.option(\"\"inferSchema\"\", infer_schema) \\.option(\"\"header\"\", first_row_is_header) \\\"\n",
        ".option(\"sep\", delimiter) \\\n",
        ".load(file_location)\n",
        "\n",
        "display(df)\n",
        "df.createOrReplaceTempView(\"tempdata\")"
      ],
      "metadata": {
        "id": "kCNKB1hZy8SD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}